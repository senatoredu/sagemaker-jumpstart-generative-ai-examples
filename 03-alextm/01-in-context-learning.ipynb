{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3ca1226-f997-41d8-b383-54367ddfabf5",
   "metadata": {},
   "source": [
    "## Foundation Model - AlexTM (In-context Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e8ef52-2eaa-40e4-8568-2638925caffe",
   "metadata": {},
   "source": [
    "#### I. Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e895edbf-bdb4-4a7a-87d3-fe148dc86015",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import script_uris\n",
    "from sagemaker import image_uris \n",
    "from sagemaker import model_uris\n",
    "from datetime import datetime\n",
    "import sagemaker\n",
    "import logging\n",
    "import boto3\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e4161b-3dc9-43ff-984d-8d54df5ffdc5",
   "metadata": {},
   "source": [
    "##### Setup logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f715a34-d1af-4085-93d9-14e816b99f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b3fcad-4417-42d8-ac9b-14840370bc41",
   "metadata": {},
   "source": [
    "##### Log versions of dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd05f5c3-35d0-4865-9eda-93c171306003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Using sageMaker version: 2.120.0]\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'[Using sageMaker version: {sagemaker.__version__}]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9422e8b9-87b8-4db8-96f5-6eaf1081375a",
   "metadata": {},
   "source": [
    "#### II. Setup essentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89e6bafa-57f6-42b3-99a4-da18eb105799",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Role => arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\n"
     ]
    }
   ],
   "source": [
    "MODEL_ID = 'pytorch-textgeneration1-alexa20b'  # this is hard-coded\n",
    "MODEL_VERSION = '*'\n",
    "INSTANCE_TYPE = 'ml.p3.8xlarge'\n",
    "INSTANCE_COUNT = 1\n",
    "IMAGE_SCOPE = 'inference'\n",
    "MODEL_DATA_DOWNLOAD_TIMEOUT = 3600  # in seconds\n",
    "CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT = 3600\n",
    "EBS_VOLUME_SIZE = 256  # in GB\n",
    "\n",
    "# set up roles and clients \n",
    "client = boto3.client('sagemaker-runtime')\n",
    "ROLE = get_execution_role()\n",
    "logger.info(f'Role => {ROLE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52b7b52a-44f2-4506-99fb-e31b4f49cff3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Endpoint name: js-pytorch-textgeneration1-alexa20b-ep-2023-02-15-03-27-03-817\n"
     ]
    }
   ],
   "source": [
    "endpoint_name = name_from_base(f'js-{MODEL_ID}-ep')\n",
    "logger.info(f'Endpoint name: {endpoint_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1b377e-0460-42dd-9f0c-9c586de5bdaa",
   "metadata": {},
   "source": [
    "#### III. Retrieve artifacts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc9b328a-1dc0-4c88-accf-971917b8a6c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deploy image URI => 763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38\n"
     ]
    }
   ],
   "source": [
    "deploy_image_uri = image_uris.retrieve(region=None, \n",
    "                                       framework=None, \n",
    "                                       image_scope=IMAGE_SCOPE, \n",
    "                                       model_id=MODEL_ID, \n",
    "                                       model_version=MODEL_VERSION, \n",
    "                                       instance_type=INSTANCE_TYPE)\n",
    "logger.info(f'Deploy image URI => {deploy_image_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13c2f847-149a-451d-9093-6aa871c3af90",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model URI => s3://jumpstart-cache-prod-us-east-1/pytorch-infer/infer-pytorch-textgeneration1-alexa20b.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_uri = model_uris.retrieve(model_id=MODEL_ID, \n",
    "                                model_version=MODEL_VERSION, \n",
    "                                model_scope=IMAGE_SCOPE)\n",
    "logger.info(f'Model URI => {model_uri}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cc8c438-0400-49b8-992f-cfc4a384614d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    'SAGEMAKER_MODEL_SERVER_TIMEOUT': str(3600),\n",
    "    'MODEL_CACHE_ROOT': '/opt/ml/model', \n",
    "    'SAGEMAKER_ENV': '1',\n",
    "    'SAGEMAKER_SUBMIT_DIRECTORY': '/opt/ml/model/code/',\n",
    "    'SAGEMAKER_PROGRAM': 'inference.py',\n",
    "    'SAGEMAKER_MODEL_SERVER_WORKERS': '1', \n",
    "    'TS_DEFAULT_WORKERS_PER_MODEL': '1', \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04805ed6-d813-4a87-8874-ee8d8e8d0da9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(image_uri=deploy_image_uri, \n",
    "              model_data=model_uri, \n",
    "              role=ROLE, \n",
    "              predictor_cls=Predictor, \n",
    "              name=endpoint_name, \n",
    "              env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dddbbf-897b-4372-b9b5-cc28cef80c57",
   "metadata": {},
   "source": [
    "#### IV. Deploy the model for real-time inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22275153-8dca-4042-97ea-728ce2447ac0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating model with name: js-pytorch-textgeneration1-alexa20b-ep-2023-02-15-03-27-03-817\n",
      "CreateModel request: {\n",
      "    \"ModelName\": \"js-pytorch-textgeneration1-alexa20b-ep-2023-02-15-03-27-03-817\",\n",
      "    \"ExecutionRoleArn\": \"arn:aws:iam::119174016168:role/service-role/AmazonSageMaker-ExecutionRole-20211014T093628\",\n",
      "    \"PrimaryContainer\": {\n",
      "        \"Image\": \"763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-inference:1.12.0-gpu-py38\",\n",
      "        \"Environment\": {\n",
      "            \"SAGEMAKER_MODEL_SERVER_TIMEOUT\": \"3600\",\n",
      "            \"MODEL_CACHE_ROOT\": \"/opt/ml/model\",\n",
      "            \"SAGEMAKER_ENV\": \"1\",\n",
      "            \"SAGEMAKER_SUBMIT_DIRECTORY\": \"/opt/ml/model/code/\",\n",
      "            \"SAGEMAKER_PROGRAM\": \"inference.py\",\n",
      "            \"SAGEMAKER_MODEL_SERVER_WORKERS\": \"1\",\n",
      "            \"TS_DEFAULT_WORKERS_PER_MODEL\": \"1\"\n",
      "        },\n",
      "        \"ModelDataUrl\": \"s3://jumpstart-cache-prod-us-east-1/pytorch-infer/infer-pytorch-textgeneration1-alexa20b.tar.gz\"\n",
      "    },\n",
      "    \"Tags\": [\n",
      "        {\n",
      "            \"Key\": \"aws-jumpstart-inference-model-uri\",\n",
      "            \"Value\": \"s3://jumpstart-cache-prod-us-east-1/pytorch-infer/infer-pytorch-textgeneration1-alexa20b.tar.gz\"\n",
      "        }\n",
      "    ]\n",
      "}\n",
      "Creating endpoint-config with name js-pytorch-textgeneration1-alexa20b-ep-2023-02-15-03-27-03-817\n",
      "Creating endpoint with name js-pytorch-textgeneration1-alexa20b-ep-2023-02-15-03-27-03-817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------!CPU times: user 292 ms, sys: 35.6 ms, total: 328 ms\n",
      "Wall time: 9min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model_predictor = model.deploy(initial_instance_count=INSTANCE_COUNT, \n",
    "                               instance_type=INSTANCE_TYPE, \n",
    "                               endpoint_name=endpoint_name, \n",
    "                               volume_size=EBS_VOLUME_SIZE, \n",
    "                               model_data_download_timeout=MODEL_DATA_DOWNLOAD_TIMEOUT, \n",
    "                               container_startup_health_check_timeout=CONTAINER_STARTUP_HEALTH_CHECK_TIMEOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c1b9be-b630-43cd-9c78-6d4b7e49e852",
   "metadata": {},
   "source": [
    "#### V. Invoke endpoint for real-time inference "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f272d6-f88a-4a46-b8af-65ad1485fa01",
   "metadata": {
    "tags": []
   },
   "source": [
    "* max_length: Model generates text until the output length (which includes the input context length) reaches max_length. If specified, it must be a positive integer.\n",
    "* num_return_sequences: Number of output sequences returned. If specified, it must be a positive integer.\n",
    "* num_beams: Number of beams used for greedy search. If specified, it must be an integer greater than or equal to num_return_sequences.\n",
    "* no_repeat_ngram_size: Model ensures that a sequence of words of no_repeat_ngram_size is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* temperature: Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If temperature -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* early_stopping: If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* do_sample: If True, model samples the next word as per the likelyhood. If specified, it must be boolean.\n",
    "* top_k: In each step of text generation, sample from only the top_k most likely words. If specified, it must be a positive integer.\n",
    "* top_p: In each step of text generation, sample from the smallest possible set of words with cumulative probability top_p. If specified, it must be a float between 0 and 1.\n",
    "* seed: Fix the randomized state for reproducibility. If specified, it must be an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45386738-e240-4bac-a019-8a9abf6f19d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# endpoint_name = '<ENTER YOUR ENDPOINT NAMER HERE>'  # IF PREVIOUSLY DEPLOYED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e30710eb-8c09-49b1-a90f-42b024aef7cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke(prompt, gen_config):\n",
    "    payload = {'text_inputs': prompt}\n",
    "    payload = json.dumps(payload).encode('utf-8')\n",
    "    response = client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                      Body=payload, \n",
    "                                      ContentType='application/json')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfd9db0c-0661-4105-a976-5338062e5c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_response(response):\n",
    "    body = json.loads(response['Body'].read().decode())\n",
    "    generated_texts = body['generated_texts'][0]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77804f28-283c-4d21-9bb9-6b23b97e0454",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: COVID-19 is a deadly\n"
     ]
    }
   ],
   "source": [
    "prompt = 'COVID-19 is a deadly'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bd1f958-c03d-4259-8aac-dd1959f889ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: COVID-19 is a deadly virus\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 47.5 ms, sys: 11.1 ms, total: 58.7 ms\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'num_beams': 5, \n",
    "              'seed': 123, \n",
    "              'no_repeat_ngram_size': 2} \n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7286b4ee-a38a-4b03-8560-c792f36a4fbf",
   "metadata": {},
   "source": [
    "## N-shot Learning via In-context Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08fb137-15d5-4c87-9ae1-2b7df827057e",
   "metadata": {},
   "source": [
    "### A. Zero-shot Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010a070d-3665-4a61-925b-b142de8fa727",
   "metadata": {},
   "source": [
    "#### 1. Extract Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa1a3c89-f7a0-4664-ab32-a2bd58ba6daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "context = \"\"\"Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8343002f-e787-4603-95a9-76a1abe22e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = 'who got cheated?'\n",
    "answer = 'crow'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "92c8c8ae-61eb-4eb5-9817-5fc43980100a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM]Context:Once, a cunning fox saw a crow with a piece of cheese in its beak sitting on a branch. The fox devised a plan and flattered the crow, causing the crow to caw with delight, dropping the cheese which the fox quickly snatched up and ran away. The crow learned a valuable lesson and never trusted the fox again.<br>Question:who got cheated?<br>Answer:\n"
     ]
    }
   ],
   "source": [
    "prompt = f'[CLM]Context:{context}<br>Question:{question}<br>Answer:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e96974a-f0a9-479d-8606-e918d324d95d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: The crow.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.55 ms, sys: 310 µs, total: 5.86 ms\n",
      "Wall time: 1.59 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 50, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "response = response.split('<br>')[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4dd683-2489-4430-9742-e3b06803fbfd",
   "metadata": {},
   "source": [
    "### 2. Natural Language Inference (NLI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e86b04e-4e99-4657-8893-f1e112b5dfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "review = 'I hated the movie. Thoroughly disappointing for a sequel.'\n",
    "sentiment = 'Sentiment(Good, Bad)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a802cdcd-3df6-4543-b7f5-276d8912030e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM]Review:I hated the movie. Thoroughly disappointing for a sequel.\n",
      "Sentiment(Good, Bad):\n"
     ]
    }
   ],
   "source": [
    "prompt = f'[CLM]Review:{review}\\n{sentiment}:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1dd681f-cc51-4c10-be3b-e6902a5acc83",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: I hated the movie. Thoroughly disappointing for a sequel. Rating: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.5 ms, sys: 21 µs, total: 5.52 ms\n",
      "Wall time: 1.52 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 50, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83c152d-577b-46e3-bf06-184b7791f180",
   "metadata": {},
   "source": [
    "### B. One-shot Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0317c3cf-a42a-4019-9493-2d37a773cade",
   "metadata": {},
   "source": [
    "#### 1. Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4583aaa-8a52-4b9e-a0dc-80daec2594a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_article = 'I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!'\n",
    "train_summary = 'I love apples. They are healthy!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd1968b7-83a1-4f93-a8ab-0b679be631f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_article = 'I hate oranges especially the bitter ones. They are high in citric acid and they give me heart burns.'\n",
    "test_summary = 'I hate oranges. They are bad for my heart burn.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3aabb233-7e74-414a-a92b-aa8784157411",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM]article: I love apples especially the large juicy ones. Apples are a great source of vitamins and fiber. An apple a day keeps the doctor away!\n",
      "summary:I love apples. They are healthy!\n",
      "article: I hate oranges especially the bitter ones. They are high in citric acid and they give me heart burns.\n",
      "summary:\n"
     ]
    }
   ],
   "source": [
    "prompt = f'[CLM]article: {train_article}\\nsummary:{train_summary}\\narticle: {test_article}\\nsummary:'\n",
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad5e8db9-0a75-4467-856f-f406f97b0651",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: I hate oranges. They are high in citric acid. article: I love\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.64 ms, sys: 1.13 ms, total: 5.77 ms\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 50, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff845d5-b444-45dd-9ca6-bf220e187597",
   "metadata": {},
   "source": [
    "#### II. Natural Language Generation (NLG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fac2d5f-bfdd-4b36-abff-2accccad30a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'name[The Punter], eat_type[Indian], price_range[cheap]'\n",
    "train_out = 'The Punter provides Indian food in the cheap price range.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0ca8abf-d332-4def-ab7d-c51ec3aab60d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_inp = 'name[Blue Spice], eatType[coffee shop], price_range[expensive]'\n",
    "test_out = 'Blue Spice is a coffee shop that is a bit expensive.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "42c655a7-983b-4a41-a8ef-710a9df1ac4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    f\"[CLM] {train_inp} ==> \"\n",
    "    f\"sentence describing the place: {train_out} ; \"\n",
    "    f\"{test_inp} ==> sentence describing the place:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abdb3a53-99df-4a8b-aafb-d12fbbc07f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM] name[The Punter], eat_type[Indian], price_range[cheap] ==> sentence describing the place: The Punter provides Indian food in the cheap price range. ; name[Blue Spice], eatType[coffee shop], price_range[expensive] ==> sentence describing the place:\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c63f543-9912-479a-8efa-3f970d1b69b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: Blue Spice provides coffee shop food in the expensive price range.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6.05 ms, sys: 1.12 ms, total: 7.17 ms\n",
      "Wall time: 1.58 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 100, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "response = response.split(';')[0].strip()\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab656614-81fa-49f1-ab00-3a5cb214fe2e",
   "metadata": {},
   "source": [
    "#### Let's flip the input and output formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bc632d1-f5b4-4bd7-9dfb-cc1b6ef07563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'The Punter provides Indian food in the cheap price range.'\n",
    "train_out = 'name[The Punter], eat_type[Indian], price_range[cheap]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d74d77db-4ce6-4883-96ab-cf41a98cc2a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_inp = 'Blue Spice is a coffee shop that is a bit pricy.'\n",
    "test_out = 'name[Blue Spice], eat_type[coffee shop], price_range[pricy]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed2e7daf-f173-4c34-a27e-808a16ee9111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    f\"[CLM] {train_inp} ==> {train_out}\\n\"\n",
    "    f\"{test_inp} ==>\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9e9234f-e219-40e6-a107-45cb6796e679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM] The Punter provides Indian food in the cheap price range. ==> name[The Punter], eat_type[Indian], price_range[cheap]\n",
      "Blue Spice is a coffee shop that is a bit pricy. ==>\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "407c8ae3-6e77-4138-9ddf-11566df0d7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: name[Blue Spice], eat_type[Coffee], price_range[\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.14 ms, sys: 626 µs, total: 5.76 ms\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 50, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abd5ef-c7e6-4bf9-b691-c037dcb87683",
   "metadata": {},
   "source": [
    "III. Machine Translation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acac5c87-205d-44b2-9a88-0d127a55f790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_inp = 'Das Parlament erhebt sich zu einer Schweigeminute.'\n",
    "train_out = \"The House rose and observed a minute' s silence\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f4742af8-242d-49d6-afda-ec8afde4713a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_inp = 'Kleingärtner bewirtschaften den einstigen Grund von Bauern.'\n",
    "test_out = 'Allotment holders cultivate the soil of former farmers.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bff0188-ca4d-4901-bcd8-d7d7f6d3b1ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    f\"[CLM] Sentence: {train_inp}; \"\n",
    "    f\"Translation in English: {train_out}; \"\n",
    "    f\"Sentence: {test_inp}; \"\n",
    "    \"Translation in English:\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f4e9fbab-6e63-41c9-82d5-1c6d13c6c536",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prompt: [CLM] Sentence: Das Parlament erhebt sich zu einer Schweigeminute.; Translation in English: The House rose and observed a minute' s silence; Sentence: Kleingärtner bewirtschaften den einstigen Grund von Bauern.; Translation in English:\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Prompt: {prompt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09990c3d-eec0-4b83-8bc2-3db4deb76281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Response: Small farmers cultivate the land once owned by farmers\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.61 ms, sys: 337 µs, total: 5.95 ms\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "gen_config = {'do_sample': True, 'max_length': 50, 'top_k': 50}\n",
    "response = invoke(prompt, gen_config)\n",
    "response = parse_response(response)\n",
    "response = response.split(';')[0]\n",
    "logger.info(f'Response: {response}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30698261-2af5-4f38-9d2a-3d4e9766414d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
